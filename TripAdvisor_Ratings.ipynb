{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej_TA7MQAz54",
        "outputId": "3bed6aaa-d7c0-421a-c98d-26b99d9ec623"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered URLs with the keyword in the domain name:\n",
            "\n",
            "DataFrame:\n",
            "Empty DataFrame\n",
            "Columns: [Domain URLs]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "from googlesearch import search\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def get_domain_urls(query, keyword, num_results=10):\n",
        "    \"\"\"\n",
        "    Fetch URLs containing the root keyword in the domain name.\n",
        "\n",
        "    Parameters:\n",
        "        query (str): Search query keyword.\n",
        "        keyword (str): Root keyword to match in domain names.\n",
        "        num_results (int): Number of search results to fetch per query.\n",
        "\n",
        "    Returns:\n",
        "        list: Filtered list of URLs with the keyword in the domain name.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Perform Google search\n",
        "        url_list = list(search(query, tld=\"com\", num=num_results, stop=num_results, pause=2))\n",
        "\n",
        "        # Filter URLs to ensure keyword is in the domain name (not path or query string)\n",
        "        filtered_urls = [\n",
        "            url for url in url_list\n",
        "            if re.search(rf\"\\b{keyword}\\b\", url.split(\"//\")[-1].split(\"/\")[0], re.IGNORECASE)\n",
        "        ]\n",
        "        return filtered_urls\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while searching for '{query}': {e}\")\n",
        "        return []\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    keyword = \"physiocare\"  # Root keyword to search for in domain names\n",
        "    num_results_per_query = 10\n",
        "    search_query = keyword  # Modify if you want additional terms, e.g., \"physiocare services\"\n",
        "\n",
        "    # Use ThreadPoolExecutor for parallel processing (optional for batch queries)\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        future = executor.submit(get_domain_urls, search_query, keyword, num_results_per_query)\n",
        "        urls = future.result()\n",
        "\n",
        "    # Display results\n",
        "    print(\"Filtered URLs with the keyword in the domain name:\")\n",
        "    for url in urls:\n",
        "        print(url)\n",
        "\n",
        "    # Save results to a DataFrame\n",
        "    df = pd.DataFrame({'Domain URLs': urls})\n",
        "    print(\"\\nDataFrame:\")\n",
        "    print(df)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XFa4K3OL_kQ"
      },
      "outputs": [],
      "source": [
        "!pip install google --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2EgiOBVMEpx",
        "outputId": "a2cb277b-fc51-45bb-aefc-ea8fc05b28fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5833\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from googlesearch import search\n",
        "import random\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'}\n",
        "\n",
        "# Loading Data\n",
        "df = pd.read_excel('Trip_Advisor_Scraper.xlsx', sheet_name=0)  # You have to upload the sheet before running this code\n",
        "df = df[5500:]\n",
        "query_list = df['SEARCH_TERM'].to_list()\n",
        "print(len(query_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QK39lizMbl_7"
      },
      "outputs": [],
      "source": [
        "# You have to use a proxy rotation service to get the 'proxy_10sec.txt' file\n",
        "\n",
        "\n",
        "file_path = 'proxy_10sec.txt'\n",
        "with open(file_path, 'r') as file:\n",
        "    proxy_addresses = [line.strip() for line in file]\n",
        "proxy_list = proxy_addresses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgqSrT3MrFAl",
        "outputId": "0c1e16d4-e61a-4301-ec16-bf9518d3b21d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       Website                                                URL\n",
            "0  carpetclean          https://www.markscarpetcleaningomaha.com/\n",
            "1  carpetclean               https://www.carpetcleaningomaha.com/\n",
            "2  carpetclean         https://affordablecarpetcleaningomaha.com/\n",
            "3  carpetclean  https://begreencarpetcleaning.com/service-area...\n",
            "4  carpetclean                    https://a-1-carpetcleaning.com/\n",
            "5  carpetclean                           https://carpetclean.com/\n",
            "6  carpetclean                     https://carpetcleaner-usa.com/\n",
            "7  carpetclean                   http://www.tlccarpetcleaning.us/\n",
            "8  carpetclean  https://www.wecancarpetcleaningservicesomaha.c...\n",
            "9  carpetclean  https://www.facebook.com/markscarpetcleaningom...\n"
          ]
        }
      ],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from googlesearch import search  # Make sure you have googlesearch-python installed\n",
        "import pandas as pd\n",
        "\n",
        "# List of website names to search for\n",
        "website_names = [\n",
        "    \"carpetclean\",\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "def get_website_urls(query, num_results=50):\n",
        "    \"\"\"\n",
        "    Fetch multiple URLs for a given query using Google search.\n",
        "\n",
        "    Parameters:\n",
        "        query (str): The search query.\n",
        "        num_results (int): The number of results to fetch per query.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of URLs found for the query.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Perform Google search and fetch URLs\n",
        "        url_list = list(search(query, tld=\"co.in\", num=num_results, stop=num_results, pause=4))\n",
        "        return url_list\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while searching for '{query}': {e}\")\n",
        "        return []\n",
        "\n",
        "# Use ThreadPoolExecutor for parallel processing\n",
        "with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    # Submit tasks to the executor for each query\n",
        "    futures = {executor.submit(get_website_urls, query): query for query in website_names}\n",
        "\n",
        "    # Wait for all tasks to complete and collect results\n",
        "    for future in futures:\n",
        "        query = futures[future]\n",
        "        try:\n",
        "            urls = future.result()\n",
        "            results.append({'Website': query, 'URLs': urls})\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred for query '{query}': {e}\")\n",
        "            results.append({'Website': query, 'URLs': []})\n",
        "\n",
        "# Flatten the results into a DataFrame\n",
        "data = []\n",
        "for result in results:\n",
        "    for url in result['URLs']:\n",
        "      if website_names[0] in str(url):\n",
        "          data.append({'Website': result['Website'], 'URL': url})\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huk5uci7MiV7"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "def get_tripadvisor_url(query, proxy):\n",
        "  \"\"\"PLEASE MAKE AT MOST 500 REQUEST PER EXECUTION SO AS NOT TO GET BLOCKED BY GOOGLE DUE TO TOO MANY REQUESTS\"\"\"\n",
        "  try:\n",
        "        url_list = list(search(query, tld=\"co.in\", num=10, stop=10, pause=4))[:10]\n",
        "        if url_list:\n",
        "          url = [x for x in url_list if 'Hotel_Review' in x]\n",
        "          url = url[0] if url else url_list[0]\n",
        "        else:\n",
        "            url = None\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    url = None\n",
        "  return url\n",
        "with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    # Submit the tasks (get_tripadvisor_url function) to the executor\n",
        "    futures = [executor.submit(get_tripadvisor_url, query, random.choice(proxy_list)) for query in query_list]\n",
        "\n",
        "    # Wait for all tasks to complete and retrieve results\n",
        "    for future in futures:\n",
        "        # Append the results from each task to the list\n",
        "        results.append(future.result())\n",
        "df['WEBSITE'] = results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bajLxPzHAOYN"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import random\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "selected_proxy = random.choice(proxy_list)\n",
        "ratings = [\"Excellent\", \"Very Good\", \"Average\", \"Poor\", \"Terrible\"]\n",
        "pattern = \"(\" + \"|\".join(ratings) + \")(\\d{1,3}(?:,\\d{3})?)\"\n",
        "\n",
        "def scrape_sheet(url):\n",
        "    rating_dict = {}\n",
        "    failed_urls_403 = []\n",
        "\n",
        "    def scrape_url(url):\n",
        "        try:\n",
        "            r = requests.get(url, headers=headers, proxies={'http': selected_proxy}, timeout=15)\n",
        "            if r.status_code == 200:\n",
        "                return r\n",
        "            elif r.status_code == 403:\n",
        "                failed_urls_403.append(url)\n",
        "            else:\n",
        "                print(r.status_code)\n",
        "        except Exception as e:\n",
        "            print(f\"Error occurred while fetching URL: {url}. Error message: {e}\")\n",
        "        return None\n",
        "\n",
        "    if 'Hotel_Review' not in str(url):\n",
        "        rating_dict['WEBSITE'] = url\n",
        "        rating_dict['TOTAL_REVIEWS'] = 'No specified Hotel from this URL'\n",
        "        rating_dict['EXCELLENT'] = 'No specified Hotel from this URL'\n",
        "        rating_dict['VERY_GOOD'] = 'No specified Hotel from this URL'\n",
        "        rating_dict['AVERAGE'] = 'No specified Hotel from this URL'\n",
        "        rating_dict['POOR'] = 'No specified Hotel from this URL'\n",
        "        rating_dict['TERRIBLE'] = 'No specified Hotel from this URL'\n",
        "        return rating_dict\n",
        "\n",
        "    r = scrape_url(url)\n",
        "\n",
        "    try:\n",
        "        while r is None and failed_urls_403:\n",
        "            url = failed_urls_403.pop(0)\n",
        "            r = scrape_url(url)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred while retrying failed URL: {url}. Error message: {e}\")\n",
        "\n",
        "    if r:\n",
        "        rating_dict['WEBSITE'] = url\n",
        "        soup = BeautifulSoup(r.content, 'html.parser')\n",
        "        ss = soup.find('div', class_='DWitr')\n",
        "\n",
        "        s = ''  # Initialize s with a default value\n",
        "\n",
        "        try:\n",
        "            if ss:\n",
        "                s = ss.text\n",
        "        except AttributeError:\n",
        "            try:\n",
        "                while ss is None:\n",
        "                    r = requests.get(url, headers=headers, proxies={'http': selected_proxy, 'http': selected_proxy}, timeout=15)\n",
        "                    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "                    ss = soup.find('div', class_='DWitr')\n",
        "            except Exception as e:\n",
        "                print(f\"Error occurred while finding review section in URL: {url}. Error message: {e}\")\n",
        "\n",
        "        matches = re.findall(pattern, s)\n",
        "\n",
        "        for match in matches:\n",
        "            rating = match[0]\n",
        "            count = match[1].replace(\",\", \"\")\n",
        "            rating_dict[rating.upper()] = count\n",
        "\n",
        "        if not matches:\n",
        "            rating_dict['EXCELLENT'] = '0'\n",
        "            rating_dict['VERY GOOD'] = '0'\n",
        "            rating_dict['AVERAGE'] = '0'\n",
        "            rating_dict['POOR'] = '0'\n",
        "            rating_dict['TERRIBLE'] = '0'\n",
        "\n",
        "        total_reviews = sum(int(value) for key, value in rating_dict.items() if key != 'TOTAL_REVIEWS' and value.isdigit())\n",
        "        rating_dict['TOTAL_REVIEWS'] = str(total_reviews)\n",
        "        rating_dict = {'WEBSITE': rating_dict['WEBSITE'], 'TOTAL_REVIEWS': str(total_reviews), **rating_dict}\n",
        "        return rating_dict\n",
        "    else:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SedHKK2xoO--"
      },
      "outputs": [],
      "source": [
        "full_list = []\n",
        "for i in results:\n",
        "  full_list.append(scrape_sheet(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHMJHiGHMy5p",
        "outputId": "bc91cfd2-eb8a-4565-a0a9-cac2aaeec911"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "333\n"
          ]
        }
      ],
      "source": [
        "df2  = pd.DataFrame(full_list)\n",
        "merged_df = pd.merge(df, df2, on=\"WEBSITE\", how=\"inner\")\n",
        "merged_df.drop(columns=['TOTAL_REVIEWS_x', 'EXCELLENT_x', 'VERY_GOOD_x','VERY_GOOD_y','AVERAGE_x', 'POOR_x', 'TERRIBLE_x'], inplace=True)\n",
        "merged_df.rename(columns={\n",
        "    'TOTAL_REVIEWS_y': 'TOTAL_REVIEWS',\n",
        "    'EXCELLENT_y': 'EXCELLENT',\n",
        "    'AVERAGE_y': 'AVERAGE',\n",
        "    'POOR_y': 'POOR',\n",
        "    'TERRIBLE_y': 'TERRIBLE'\n",
        "}, inplace=True)\n",
        "\n",
        "df_no_duplicates = merged_df.drop_duplicates()\n",
        "print(len(df_no_duplicates))\n",
        "\n",
        "\n",
        "df_no_duplicates.to_csv('Rating_Data.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOI1HCDx6aJl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}