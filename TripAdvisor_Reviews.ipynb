{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bYDZ_9EEcQ2y"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'proxy_10sec.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Reading proxies\u001b[39;00m\n\u001b[1;32m     15\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproxy_10sec.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     17\u001b[0m     proxy_addresses \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file]\n\u001b[1;32m     18\u001b[0m proxy_list \u001b[38;5;241m=\u001b[39m proxy_addresses\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'proxy_10sec.txt'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from googlesearch import search\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Reading proxies\n",
    "file_path = 'proxy_10sec.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    proxy_addresses = [line.strip() for line in file]\n",
    "proxy_list = proxy_addresses\n",
    "\n",
    "date_pattern = r'\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s\\d{4}\\b'\n",
    "rating_pattern = r'bubble_(\\d{2})'\n",
    "\n",
    "selected_proxy = random.choice(proxy_list)\n",
    "\n",
    "failed_urls = []\n",
    "\n",
    "def scrape_reviews(url, base_url):\n",
    "    scraped_data = []\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, proxies={'http': selected_proxy, 'http': selected_proxy}, timeout=20)\n",
    "        r.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "        s = BeautifulSoup(r.content, 'html.parser')\n",
    "        reviews_card = s.find_all('div', class_='YibKl MC Gi z Z BB pBbQr lqYst')\n",
    "        for i in reviews_card:\n",
    "            review_data = {}\n",
    "            review_data['TRIP_ADVISOR_HREF'] = base_url\n",
    "            date = i.find('div', class_='cRVSd').text\n",
    "            date_match = re.search(date_pattern, date)\n",
    "            if date_match:\n",
    "                review_data['REVIEW_DATE'] = date_match.group(0)\n",
    "            else:\n",
    "                review_data['REVIEW_DATE'] = \"No date found.\"\n",
    "\n",
    "            rating = i.find('div', class_='Hlmiy F1').find('span')\n",
    "            match = re.search(rating_pattern, str(rating))\n",
    "            if match:\n",
    "                review_data['RATING'] = int(match.group(1)) // 10\n",
    "            else:\n",
    "                review_data['RATING'] = \"Rating value not found.\"\n",
    "\n",
    "            review_data['REVIEW_TITLE'] = i.find('div', class_='KgQgP MC _S b S6 H5 _a').text\n",
    "            review_data['REVIEW_TEXT'] = i.find('span', class_='QewHA H4 _a').text\n",
    "            scraped_data.append(review_data)\n",
    "    except RequestException as e:\n",
    "        failed_urls.append(url)\n",
    "    return scraped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dM_Ynrt4shdh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_reviews_data = []\n",
    "df = pd.read_csv('Ratings.csv')\n",
    "df = df[5500:]\n",
    "print(len(df))\n",
    "reviews_list = df['TOTAL_REVIEWS'].to_list()\n",
    "href_list= df['WEBSITE'].to_list()\n",
    "\n",
    "\n",
    "for a, b in zip(reviews_list, href_list):\n",
    "    # this checks if is a number\n",
    "    if isinstance(a, str) and (a.isalpha() or a.isdigit()):\n",
    "        if int(a) < 30:\n",
    "            # checks if review is less than 30\n",
    "            reviews_index = b.find('-Reviews-')\n",
    "            if reviews_index != -1:\n",
    "                base_url = b[:reviews_index + 9]  # Include '-Reviews-' and the following character\n",
    "                review_page_number = b[reviews_index + 9:-5]  # Exclude '.html' at the end\n",
    "                for j in range(0, int(a), 10):\n",
    "                    modified_url = f\"{base_url}or{j}-{review_page_number}.html\"\n",
    "                    # Call the scrape_reviews function and append its result to the all_reviews_data list\n",
    "                    all_reviews_data.extend(scrape_reviews(modified_url, b))\n",
    "        else:\n",
    "            reviews_index = b.find('-Reviews-')\n",
    "            if reviews_index != -1:\n",
    "                base_url = b[:reviews_index + 9]  # Include '-Reviews-' and the following character\n",
    "                review_page_number = b[reviews_index + 9:-5]  # Exclude '.html' at the end\n",
    "                # Generate URLs with different offsets using f-strings and range function\n",
    "                for j in range(0, 30, 10):\n",
    "                    modified_url = f\"{base_url}or{j}-{review_page_number}.html\"\n",
    "                    # Call the scrape_reviews function and append its result to the all_reviews_data list\n",
    "                    all_reviews_data.extend(scrape_reviews(modified_url, b))\n",
    "\n",
    "        while failed_urls:\n",
    "            url = failed_urls.pop(0)\n",
    "            all_reviews_data.extend(scrape_reviews(url, b))\n",
    "    else:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vl0YjRx1txE7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df2 = pd.DataFrame(all_reviews_data)\n",
    "df2.to_csv('last.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLFYMBDc_20O",
    "outputId": "1a6a9204-8231-40cf-e75f-6f2041cddc3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "unique_values = pd.unique(df2['TRIP_ADVISOR_HREF'])\n",
    "# Print unique values\n",
    "print(len(unique_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Pr1XO4_KiMX",
    "outputId": "27600e44-ba51-4332-bb14-63b8fc956e26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.tripadvisor.com/Hotel_Review-g55822-d613650-reviews-budget_inn_fairfield-fairfield_texas.html']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Ratings.csv')\n",
    "df = df[5500:]\n",
    "reviews_list = df['TOTAL_REVIEWS'].to_list()\n",
    "href_list= df['WEBSITE'].to_list()\n",
    "t_list = []\n",
    "for a, b in zip(reviews_list, href_list):\n",
    "    # this checks if is a number\n",
    "    if isinstance(a, str) and (a.isalpha() or a.isdigit()):\n",
    "      if int(a) >0:\n",
    "        t_list.append(b)\n",
    "c = [x for x in t_list if x not in unique_values]\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "affK450nXAaA",
    "outputId": "c47bda67-8943-493f-f1b9-a22780805459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286\n",
      "285\n"
     ]
    }
   ],
   "source": [
    "print(len(set(t_list)))\n",
    "print(len(unique_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5AANgah3xjf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vh9QegMJ3xgM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UIU0KjvR3xdt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkXugt4a3xbP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81hQbnbp3xZB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QiCJ5oG13xUn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7Hf1DnD3xRT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SCpijG63xOd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XbOlXTTH3xME"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Psg_GcR5y-oV",
    "outputId": "21a79a3b-b409-41d1-9bd6-10e043ae27f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_0-10.csv done\n",
      "R_10-100.csv done\n",
      "R_100-200.csv done\n",
      "R_200-500.csv done\n",
      "R_500-1000.csv done\n",
      "R_1000-1500.csv done\n",
      "R_1500-2000.csv done\n",
      "R_2000-2500.csv done\n",
      "R_2500-2510.csv done\n",
      "R_2510-2530_real.csv done\n",
      "R_2530-2600.csv done\n",
      "R_2600-3000.csv done\n",
      "R_3000-3500.csv done\n",
      "R_3500-4000.csv done\n",
      "R_4000-4500.csv done\n",
      "R_4500-5000.csv done\n",
      "R_5000-5500.csv done\n",
      "R_5500-last.csv done\n",
      "last.csv done\n",
      "125694\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# List of CSV file names\n",
    "csv_files = [\n",
    "    'R_0-10.csv',\n",
    "    'R_10-100.csv',\n",
    "    'R_100-200.csv',\n",
    "    'R_200-500.csv',\n",
    "    'R_500-1000.csv',\n",
    "    'R_1000-1500.csv',\n",
    "    'R_1500-2000.csv',\n",
    "    'R_2000-2500.csv',\n",
    "    'R_2500-2510.csv',\n",
    "    'R_2510-2530_real.csv',\n",
    "    'R_2530-2600.csv',\n",
    "    'R_2600-3000.csv',\n",
    "    'R_3000-3500.csv',\n",
    "    'R_3500-4000.csv',\n",
    "    'R_4000-4500.csv',\n",
    "    'R_4500-5000.csv',\n",
    "    'R_5000-5500.csv',\n",
    "    'R_5500-last.csv',\n",
    "    'last.csv'\n",
    "]\n",
    "\n",
    "# Read all CSV files into DataFrames\n",
    "dfs = []\n",
    "for file_name in csv_files:\n",
    "    with open(file_name, 'r') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        header = next(csv_reader)  # Read the header\n",
    "        data = [row for row in csv_reader]  # Read the remaining rows\n",
    "        df = pd.DataFrame(data, columns=header)\n",
    "        dfs.append(df)\n",
    "    print(file_name, 'done')\n",
    "\n",
    "# Concatenate all DataFrames vertically\n",
    "result = pd.concat(dfs, ignore_index=True)\n",
    "print(len(result))\n",
    "\n",
    "result.to_csv('Reviews.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b-jHiVf-3brV",
    "outputId": "5f315ed2-634e-4e73-c5a4-55a6ac460d00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the first CSV file\n",
    "df1 = pd.read_csv('Ratings.csv')\n",
    "\n",
    "# Read the second CSV file\n",
    "df2 = result\n",
    "\n",
    "# Create an Excel writer object\n",
    "with pd.ExcelWriter('RevandRatings_.xlsx') as writer:\n",
    "    # Write the first DataFrame to the Excel file as Sheet1\n",
    "    df1.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "\n",
    "    # Write the second DataFrame to the Excel file as Sheet2\n",
    "    df2.to_excel(writer, sheet_name='Sheet2', index=False)\n",
    "\n",
    "print(\"Excel file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKYVU_VP3qx4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
